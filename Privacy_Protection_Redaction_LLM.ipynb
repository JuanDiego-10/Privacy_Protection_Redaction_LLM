{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "3_b2AWoS2H0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9670103-62d2-4550-916c-49e51f98f36f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IyrgGuW4DlOX",
        "outputId": "1ff57345-1043-46c6-cfb1-3f024d248198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "model_name = \"iiiorg/piiranha-v1-detect-personal-information\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def mask_pii(text, aggregate_redaction=True):\n",
        "    # Tokenize input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Get the model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the predicted labels\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "    # Convert token predictions to word predictions\n",
        "    encoded_inputs = tokenizer.encode_plus(text, return_offsets_mapping=True, add_special_tokens=True)\n",
        "    offset_mapping = encoded_inputs['offset_mapping']\n",
        "\n",
        "    masked_text = list(text)\n",
        "    is_redacting = False\n",
        "    redaction_start = 0\n",
        "    current_pii_type = ''\n",
        "\n",
        "    for i, (start, end) in enumerate(offset_mapping):\n",
        "        if start == end:  # Special token\n",
        "            continue\n",
        "\n",
        "        label = predictions[0][i].item()\n",
        "        if label != model.config.label2id['O']:  # Non-O label\n",
        "            pii_type = model.config.id2label[label]\n",
        "            if not is_redacting:\n",
        "                is_redacting = True\n",
        "                redaction_start = start\n",
        "                current_pii_type = pii_type\n",
        "            elif not aggregate_redaction and pii_type != current_pii_type:\n",
        "                # End current redaction and start a new one\n",
        "                apply_redaction(masked_text, redaction_start, start, current_pii_type, aggregate_redaction)\n",
        "                redaction_start = start\n",
        "                current_pii_type = pii_type\n",
        "        else:\n",
        "            if is_redacting:\n",
        "                apply_redaction(masked_text, redaction_start, end, current_pii_type, aggregate_redaction)\n",
        "                is_redacting = False\n",
        "\n",
        "    # Handle case where PII is at the end of the text\n",
        "    if is_redacting:\n",
        "        apply_redaction(masked_text, redaction_start, len(masked_text), current_pii_type, aggregate_redaction)\n",
        "\n",
        "    return ''.join(masked_text)\n",
        "\n",
        "def apply_redaction(masked_text, start, end, pii_type, aggregate_redaction):\n",
        "    for j in range(start, end):\n",
        "        masked_text[j] = ''\n",
        "    if aggregate_redaction:\n",
        "        masked_text[start] = '[redacted]'\n",
        "    else:\n",
        "        masked_text[start] = f'[{pii_type}]'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "example_text = \"Hola, me llamo Juan y vivo en Calle del Sol, 45, Madrid, España.\"\n",
        "\n",
        "print(\"Aggregated redaction:\")\n",
        "masked_example_aggregated = mask_pii(example_text, aggregate_redaction=True)\n",
        "print(masked_example_aggregated)\n",
        "\n",
        "print(\"\\nDetailed redaction:\")\n",
        "masked_example_detailed = mask_pii(example_text, aggregate_redaction=False)\n",
        "print(masked_example_detailed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2A1X1hi9BMi",
        "outputId": "fd4dbafd-77f9-4fa9-a336-5f569a2a87e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated redaction:\n",
            "Hola, me llamo[redacted] en[redacted][redacted], España.\n",
            "\n",
            "Detailed redaction:\n",
            "Hola, me llamo[I-GIVENNAME] en[I-STREET][I-BUILDINGNUM], España.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "90AQ_I4CLfWW"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}
